alpha = 0.6) +
theme_bw() +
scale_x_log10() +
xlab('log x') +
ylab('count') +
geom_density(alpha=0.2, fill='pink')
```
We can also plot the variable on a log-log scale (this is where both x- and y- axes are log transformed). On a log-log scale a power law follows a straight line.
```{r log_power_3}
# plot a histogram of both x-axis and y-axis log-tranformed df1
ggplot(df.power, aes(x)) + geom_histogram(bins=40,
color = 'black',
fill = 'grey',
alpha = 0.6) +
scale_y_log10() +
scale_x_log10() +
theme_bw() +
xlab('log10 x')+
ylab('log10 count')
```
### Log Normal
The problem with using a lin-lin plot (which is the most common type of plot used in social science research) is that different functions can look very similar. Take a look at this histogram of a log-normally distributed variable on a lin-lin plot.
```{r log_normal_1}
# plot a histogram of df.lognormal
ggplot(df.lognormal, aes(x)) + geom_histogram(bins=25,
color = 'black',
fill = 'grey',
alpha = 0.6)
```
From just the lin-lin plot you might think that it is a power law - but once we log transform the data, we can see that it actually follows a log-normal distribution:
```{r log_normal_2}
# plot a histogram of the log-transformed variable
ggplot(df.lognormal, aes(x)) + geom_histogram(bins=25,
color = 'black',
fill = 'grey',
alpha = 0.6) +
scale_x_log10() +
xlab('log x') +
ylab('count')
```
### Exponential
Now, take a look at an exponentially distributed variable:
```{r log_exp_1}
# plot a histogram of df3
ggplot(df.exp, aes(x)) + geom_histogram(bins=25,
color = 'black',
fill = 'grey',
alpha = 0.6)
```
Once again, it looks like another long-tailed distribution. Now look at it with the x-axis log transformed (log-lin plot).
```{r log_exp_2}
# plot a histogram of the log-transformed df3
ggplot(df.exp, aes(x)) + geom_histogram(bins=40,
color = 'black',
fill = 'grey',
alpha = 0.6) +
scale_x_log10() +
xlab('log x') +
ylab('count')
```
And, in a log-log plot:
```{r log_exp_3}
# plot a histogram of the log-transformed df3
ggplot(df.exp, aes(x)) + geom_histogram(bins=40,
color = 'black',
fill = 'grey',
alpha = 0.6) +
scale_x_log10() +
scale_y_log10() +
xlab('log x') +
ylab('log count')
```
## Comparing distributions
Log-normal, exponential and power functions all look very similar when plotted on a lin-lin plot. Log-normal and Exponential functions also look very similar when plotted on a log-lin plot. But they look very different when plotted on a log-log plot or a lin-log plot. Equally, power law and log-normal functions look similar on a log-log plot but very different on a log-lin plot. This might all sound a bit confusing - run this code to see how they all compare:
```{r compare-functions}
# lin - lin plots
ggplot(df.compare, aes(value)) + geom_histogram(bins=40,
color = 'black',
fill = 'grey',
alpha = 0.6) +
facet_wrap(~funz, scales = 'free') +
ggtitle('lin-lin plots')
# lin - log plots
ggplot(df.compare, aes(value)) + geom_histogram(bins=40,
color = 'black',
fill = 'grey',
alpha = 0.6) +
facet_wrap(~funz, scales = 'free') +
#scale_y_log10() +
scale_y_continuous(trans="log10") +
ggtitle('lin-log plots')
# log - log plots
ggplot(df.compare, aes(value)) + geom_histogram(bins=40,
color = 'black',
fill = 'grey',
alpha = 0.6) +
facet_wrap(~funz, scales = 'free') +
scale_x_log10() +
scale_y_log10() +
ggtitle('log-log plots')
# log - lin plots
ggplot(df.compare, aes(value)) + geom_histogram(bins=40,
color = 'black',
fill = 'grey',
alpha = 0.6) +
facet_wrap(~funz, scales = 'free') +
scale_x_log10() +
ggtitle('log-lin plots')
```
The key takehome message is that you cannot rely on just one type of transformation to understand your data; you need to use different visualizations to understand its properties and how it should best be modelled.
## Power laws revisited
Power laws are one of the most commonly occuring distributions, and a key part of this course. df.power contains variables which can be modelled with different power functions. See how they look without any transformation:
```{r power_plot}
# plot several power functions on a lin-lin plot
ggplot(df.power.1, aes(x,value)) +
geom_line(aes(color=power))
```
This plot is not particularly clear. Plotting a power function on a log-log plot makes it easier to interpret. If we observe a straight line when we plot a variable on a log-log plot then we know that it follows a power function. Different powers create different gradient lines. This is illustrated below:
```{r power_plot1}
# plot the power functions on a log-log plot
ggplot(df.power.1, aes(x,value)) +
geom_line(aes(color=power)) +
scale_x_continuous(trans = "log10") +
scale_y_continuous(trans = "log10")  #adjust trans to 'log' or 'log2' to alter the base value
# And to make the log10 plot look cool using ggplot2:
ggplot(df.power.1, aes(x,value)) +
geom_line(aes(color=power)) +
scale_x_log10() +
scale_y_log10("log of y value",
breaks = scales::trans_breaks("log10", function(x) 10^x),
labels = scales::trans_format("log10", scales::math_format(10^.x))
) +
annotation_logticks() +
xlab('log of x value') +
ggtitle('log-log plot of powers') +
theme(plot.title = element_text(lineheight=2, face="bold"))
```
## Exponents revisited (briefly)
Often, the difference between the exponential function and a power law is ignored when we are talking everyday about rates of change. BUT! they really differ in terms of their accelaration. Exponents increase far faster than powers.
This is because with a power law function the formula is: f(x) = x^b. The exponent stays constant, whilst the base increases. So if our exponent is 2 and the base ranges from 1 to 10, we get: 1, 4, 9, 16, 25, 36, 49, 64, 81, 100. With an exponential function the formula is: f(x) = b^x. The exponent increases whilst the base stays constant. So if our base is 2 and our exponent x range from 1 to 10 we get: 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024.
You can observe this in the plot below:
```{r exponent_plot1}
# plot exponent vs. power function:
ggplot(df.power.exponent, aes(x,value)) +
geom_line(aes(color=function_name)) +
theme(legend.title = element_text(colour="blue", size=10,
face="bold"))
```
Exponents have some very cool properties. Some of the nice ones are:
x^m * x^n = x^(m+n)
x^m / x^n = x^(m-n)
(x^m)^n = x^(m*n)
x^m * y^m = xy^m
x^m / y^m = (x/y)^m
x^0 = 1
## Saving your ggplots as .jpg files
Last but not least... how to save your ggplots as .jpg files so you don't have to copy and paste them manually into other docs.
```{r ggplot-save}
# First, save your plot to an object - give it an expressive name
plot.variableX = ggplot(df.exp, aes(x)) + geom_histogram(bins=40,
color = 'black',
fill = 'grey',
alpha = 0.6) +
scale_x_log10() +
scale_y_log10() +
theme_bw() +
xlab('log x') +
ylab('log count')
# Second, have a look at the plot to make sure that it's what is expected
plot.variableX
ggsave("~/Dropbox/plot.variableX.jpg", plot = plot.variableX,
width = 6, height = 6, units = 'in')
```
*End of Workshop Notes.*
options(scipen=10) # makes the output more readable by increasing the number of values before scientific notation is used.
# Load packages
fun_check_packages <- function(x){
for (i in seq(1,length(x))){
if (!x[i] %in% installed.packages()){
install.packages(x[i])}
library(x[i],character.only=TRUE)}
}
packs = c('knitr', 'ggplot2', 'LSD','dplyr','tidyr','scales', 'cowplot', 'MASS')
fun_check_packages(packs); rm(packs); rm(fun_check_packages)
# Check your working directory is set correctly:
getwd()
knitr:: opts_knit$set(root.dir = '/Users/Greyson/Desktop') # change this to the file path containing your downloaded data for this workshop
# Load the data for today's class
load('BDA_week2.RData')
# produce a histogram of x using the base R package - this is fine but we can't adjust many of the prams very easy and it's a bit ugly
hist(df.normal$x)
# produce a nicer looking histogram of x using ggplot2
ggplot(df.normal, aes(x)) + geom_histogram(color = 'black',
fill = 'grey',
alpha = 0.6) +
#theme_bw() +
ggtitle('Histogram of x')
# set the number of bins using the 'bins' argument
ggplot(df.normal, aes(x)) + geom_histogram(bins=50,
color = 'black',
fill = 'grey',
alpha = 0.6) +
#theme_bw() +
ggtitle('Histogram of x')
ggplot(df.normal, aes(x)) +
geom_histogram(aes(y= ..density.. ), # density instead of count on y-axis - otherwise the histogram and PDF will have different scales
bins=50,
color = 'black',
fill = 'grey',
alpha = 0.6) +
geom_density(alpha=0.2, fill='pink')  # Use alpha to make t
# Create a dataframe with the ecdf values for each corresponding variable
df.ecdf <- data.frame(x = sort(df.normal$x), # these are the values of our variable - sorted into order from smallest to highest
y = my_ecdf(sort(df.normal$x) )) # these are the corresponding ecdf values for each value of our variable; for each value it calculates its cumulative probability
my_ecdf <- ecdf(df.normal$x)
# Create a dataframe with the ecdf values for each corresponding variable
df.ecdf <- data.frame(x = sort(df.normal$x), # these are the values of our variable - sorted into order from smallest to highest
y = my_ecdf(sort(df.normal$x) )) # these are the corresponding ecdf values for each value of our variable; for each value it calculates its cumulative probability
# Plot
# you can add scale_x_log10() and scale_y_log10() for power laws
ggplot(data = df.ecdf, aes(x, y) ) +
geom_line() +
geom_point(color="black") +
ggtitle('ECDF of x') +
xlab('x') +
ylab('Cumulative probability')
acf.out.2 = rBDA::cor_fun(df.acf$y)
rBDA::cor_plot(acf.out.2)
acf.out.2 = rBDA::cor_fun(df.acf$y)
head(acf.out.2$cor, 10)
df.A = df.A %>%
mutate(interval = rep(1: NumIntervals, each = Interval)) %>%
mutate(interval = as.factor(interval)) %>%
mutate(time.1 = rep(1:Interval, times = NumIntervals))
ggplot(df.A, aes(x = time.1, y = y, group = interval, colour = interval)) +
geom_line() +
xlab('days') +
ggtitle('Time interval of one week') +
theme(legend.position = 'none') # removes the legend from the plot
ggplot() +
geom_line(data = tweets30,  aes(x = Times, y = Tweets, group = Location, colour = Location)) +
xlab('Times') +
ggtitle('Time interval of one week')
ggplot() +
geom_line(data = tweets15,  aes(x = Times, y = Tweets, group = Location, colour = Location)) +
xlab('Times') +
ggtitle('Time interval of one week')
ggplot() +
geom_line(data = df.A,  aes(x = time.1, y = y, group = interval, colour = interval)) +
xlab('days') +
ggtitle('Time interval of one week') +
geom_line(data = df.ave, aes(x=group, y=ave),
color = 'black', size = 2)
df.ave = aggregate(df.acf.extend$y, by=list(df.acf.extend$day), FUN=mean); rm(df.acf.extend)
df.ave = aggregate(df.acf.extend$y, by=list(df.acf.extend$day), FUN=mean); rm(df.acf.extend)
df.ave = aggregate(df.acf.extend$y, by=list(df.acf.extend$day), FUN=mean); rm(df.acf.extend)
df.acf.extend = df.acf %>%
dplyr::mutate(day = rep(seq(1,7), 40))
df.ave = aggregate(df.acf.extend$y, by=list(df.acf.extend$day), FUN=mean); rm(df.acf.extend)
ggplot() +
geom_line(data = df.A,  aes(x = time.1, y = y, group = interval, colour = interval)) +
xlab('days') +
ggtitle('Time interval of one week') +
geom_line(data = df.ave, aes(x=group, y=ave),
color = 'black', size = 2)
ggplot() +
geom_line(data = df.A,  aes(x = time.1, y = y, group = interval, colour = interval)) +
xlab('days') +
ggtitle('Time interval of one week') +
geom_line(data = df.ave, aes(x=group, y=ave),
color = 'black', size = 2)
df.ave = aggregate(df.acf.extend$y, by=list(df.acf.extend$day), FUN=mean); rm(df.acf.extend)
tend = df.acf %>%
dplyr::
df.acf.extend = df.acf %>%
dplyr::mutate(day = rep(seq(1,7), 40))
df.ave = aggregate(df.acf.extend$y, by=list(df.acf.extend$day), FUN=mean); rm(df.acf.extend)
colnames(df.ave) = c('group', 'ave')
ggplot() +
geom_line(data = df.A,  aes(x = time.1, y = y, group = interval, colour = interval)) +
xlab('days') +
ggtitle('Time interval of one week') +
geom_line(data = df.ave, aes(x=group, y=ave),
color = 'black', size = 2)
ags = 30 # just one value
lags = c(7,30,90) # several values
# calculate time window values
df.lag.window = rBDA::time_window(lags, df.lag)
# the first value 'lags' is the lags we want to calculate the values over
# df.lag is a dataframe - the first value should be a date, and the second value a numeric or integer variable (as we need to be able to calculate the mean of it)
class(df.lag$x) #Date
class(df.lag$y) #numeric
head(df.lag.window, 5)
# date shows the date in numeric form
# dep.var shows the smoothed values for the dependent variable
# window shows the time lag window
# convert numeric 'date' to Date format
df.lag.window$date = as.Date(df.lag.window$date)
thirty.days.lag = subset(df.lag.window, window == 'lag_30') # adjust to lag_7 or lag_90 for the other lag values
ggplot(thirty.days.lag, aes(date, dep.var)) +
geom_line() +
ggtitle('Time lag of 30 days')
levels(df.lag.window$window) = c("lag_0", "lag_7", "lag_30", "lag_90")
ggplot(df.lag.window, aes(date, dep.var, color = window)) +
geom_line() +
ggtitle('Several time lags')
ggplot(df.lag.window, aes(date, dep.var, color = window)) +
facet_wrap(~window) +
geom_line() +
xlab('Time') +
ylab('Variable') +
theme(axis.text.x = element_text(angle=45, hjust = 1),
legend.position = 'none')
my_ecdf <- ecdf(df.normal$x)
head(my_ecdf)
my_ecdf <- ecdf(lengthData$textLength)
df.ecdf <- data.frame(x = sort(lengthData$textLength), # these are the values of our variable - sorted into order from smallest to highest
y = my_ecdf(sort(lengthData$textLength) )) # these are the corresponding ecdf values for each value of our variable; for each value it calculates its cumulative probability
# Plot
# you can add scale_x_log10() and scale_y_log10() for power laws
ggplot(data = df.ecdf, aes(x, y) ) +
geom_line() +
geom_point(color="black") +
ggtitle('ECDF of x') +
xlab('x') +
ylab('Cumulative probability')
my_ecdf <- ecdf(df.normal$x)
head(my_ecdf)
# Create a dataframe with the ecdf values for each corresponding variable
df.ecdf <- data.frame(x = sort(df.normal$x), # these are the values of our variable - sorted into order from smallest to highest
y = my_ecdf(sort(df.normal$x) )) # these are the corresponding ecdf values for each value of our variable; for each value it calculates its cumulative probability
# Plot
# you can add scale_x_log10() and scale_y_log10() for power laws
ggplot(data = df.ecdf, aes(x, y) ) +
geom_line() +
geom_point(color="black") +
ggtitle('ECDF of x') +
xlab('x') +
ylab('Cumulative probability')
ggplot(data = df.ecdf, aes(x, y) ) +
geom_line() +
geom_point(color="black") +
ggtitle('ECDF of x') +
xlab('Length of Tweet in Words') +
ylab('Cumulative probability')
df.ecdf <- data.frame(x = sort(lengthData$textLength), # these are the values of our variable - sorted into order from smallest to highest
y = my_ecdf(sort(lengthData$textLength) )) # these are the corresponding ecdf values for each value of our variable; for each value it calculates its cumulative probability
my_ecdf <- ecdf(lengthData$textLength)
df.ecdf <- data.frame(x = sort(lengthData$textLength), # these are the values of our variable - sorted into order from smallest to highest
y = my_ecdf(sort(lengthData$textLength) )) # these are the corresponding ecdf values for each value of our variable; for each value it calculates its cumulative probability
ggplot(data = df.ecdf, aes(x, y) ) +
geom_line() +
geom_point(color="black") +
ggtitle('ECDF of x') +
xlab('Length of Tweet in Words') +
ylab('Cumulative probability')
ggplot(data = df.ecdf, aes(x, y) ) +
geom_line() +
geom_point(color="blur") +
ggtitle('ECDF of x') +
xlab('Length of Tweet in Words') +
ylab('Cumulative probability')
ggplot(data = df.ecdf, aes(x, y) ) +
geom_line() +
geom_point(color="blue") +
ggtitle('ECDF of x') +
xlab('Length of Tweet in Words') +
ylab('Cumulative probability')
ggplot(data = df.ecdf, aes(x, y) ) +
geom_line() +
geom_point(color="blue") +
ggtitle('ECDF of Words Used') +
xlab('Length of Tweet in Words') +
ylab('Cumulative probability')
ggplot(data = df.ecdf, aes(x, y) ) +
geom_line() +
geom_point(color="blue") +
ggtitle('ECDF of # Words Used') +
xlab('Length of Tweet in Words') +
ylab('Cumulative probability')
ggplot(data = df.ecdf, aes(x, y) ) +
geom_line() +
geom_point(color="blue") +
ggtitle('ECDF of # of Words Used') +
xlab('Length of Tweet in Words') +
ylab('Cumulative probability')
ggsave("ecdfWords.png")
ggplot(lengthData, aes(politicsN, textLength)) +
geom_point(color =ifelse(lengthData$politicsN>0.0, "blue", "red"), size = 0.1) +
labs(x = 'Political Leaning', y= 'Number of Words in Tweet') +
ggtitle('Political Alligance vs. Length of Tweet')
ggplot(lengthData, aes(politicsN, textLength)) +
geom_point(color =ifelse(lengthData$politicsN>0.0, "blue", "red"), size = 0.1) +
labs(x = 'Political Leaning', y= 'Number of Words in Tweet') +
ggtitle('Political Alligance vs. Length of Tweet')
ggsave("twLengthByVote.png")
View(tweetsPoltical)
df.A = df.A %>%
mutate(interval = rep(1: NumIntervals, each = Interval)) %>%
mutate(interval = as.factor(interval)) %>%
mutate(time.1 = rep(1:Interval, times = NumIntervals))
ggplot() +
geom_line(data = df.A,  aes(x = time.1, y = y, group = interval, colour = interval)) +
xlab('days') +
ggtitle('Time interval of one week') +
geom_line(data = df.ave, aes(x=group, y=ave),
color = 'black', size = 2)
ggplot(df.A, aes(x = time.1, y = y, group = interval, colour = interval)) +
geom_line() +
xlab('days') +
ggtitle('Time interval of one week') +
theme(legend.position = 'none') # removes the legend from the plot
stats::acf(df.sine$y, lag.max=1000, plot=T, type='correlation')
stats::acf(tweets15$Oregon, lag.max=1000, plot=T, type='correlation')
stats::acf(tweets15$Florida, lag.max=1000, plot=T, type='correlation')
stats::acf(tweets15$Florida, lag.max=1000, plot=Time, type='correlation')
stats::acf(tweets15$Florida, lag.max=1000, plot=T, type='correlation')
stats::acf(tweets15$Oregon, lag.max=1000, plot=T, type='correlation')
stats::acf(tweets15$Washington, lag.max=1000, plot=T, type='correlation')
stats::acf(tweets15$Colorado, lag.max=1000, plot=T, type='correlation')
stats::acf(tweets15$Vermont, lag.max=1000, plot=T, type='correlation')
head(tweets15$Florida)
tweets15 = read.csv("data/tweets15Days.csv")
tweets30 = read.csv("data/tweets30Days.csv")
tweets60 = read.csv("data/tweets60Days.csv")
stats::acf(tweets15$Florida, lag.max=1000, plot=T, type='correlation')
stats::acf(tweets15$Florida, lag.max=1000, plot=T, type='correlation')
head(tweets15$Florida)
tweets15 = read.csv("data/tweets15Days.csv")
setwd("~/Documents/Main Greyson/Oxford/Information Visualization/twitterTest")
tweets15 = read.csv("data/tweets15Days.csv")
setwd("~/Documents/Main Greyson/Oxford/Information Visualization/twitterTest")
tweets15 = read.csv("data/tweets15Days.csv")
tweets30 = read.csv("data/tweets30Days.csv")
tweets60 = read.csv("data/tweets60Days.csv")
tweets15 = read.csv("data/tweets15Days.csv")
tweets60 = read.csv("data/tweets60Days.csv")
tweets30 = read.csv("data/tweets30Days.csv")
stats::acf(tweets15$Florida, lag.max=1000, plot=T, type='correlation')
head(tweets30)
stats::acf(tweets15$Florida, lag.max=1000, plot=T, type='correlation')
stats::acf(tweets15$Oregon, lag.max=1000, plot=T, type='correlation')
function (x, lag.max = NULL, type = c("correlation", "covariance",
"partial"), plot = TRUE, na.action = na.fail, demean = TRUE,
...)
stats::acf(tweets15$Florida, lag.max=1000, plot=True, type='correlation')
stats::acf(tweets15$Florida, lag.max=1000, plot=True, type='correlation')
stats::acf(tweets15, lag.max=1000, plot=T, type='correlation')
View(tweets15)
flordia60 <- subset(tweets60$Location, Location == "Florida" )
flordia60 <- subset(tweets60$Location, tweets60$Location == "Florida" )
stats::acf(flordia60$Location, lag.max=1000, plot=T, type='correlation')
stats::acf(flordia60$Time, lag.max=1000, plot=T, type='correlation')
flordia60 <- subset(tweets60, tweets60$Location == "Florida" )
stats::acf(flordia60$Time, lag.max=1000, plot=T, type='correlation')
stats::acf(flordia60$Time, lag.max=30, plot=T, type='correlation')
stats::acf(flordia60$Time, lag.max=200, plot=T, type='correlation')
stats::acf(flordia60$Time, lag.max=1000, plot=T, type='correlation')
stats::acf(flordia60$Time, lag.max=100, plot=T, type='correlation')
stats::acf(flordia60$Time, lag.max=150, plot=T, type='correlation')
stats::acf(tweets15$Oregon, lag.max=80, plot=T, type='correlation')
stats::acf(flordia60$Time, lag.max=80, plot=T, type='correlation')
Oregon6- <- subset(tweets60, tweets60$Location == "Oregon" )
stats::acf(Oregon60$Oregon, lag.max=80, plot=T, type='correlation')
Oregon60 <- subset(tweets60, tweets60$Location == "Oregon" )
stats::acf(Oregon60$Oregon, lag.max=80, plot=T, type='correlation')
stats::acf(Oregon60$Time, lag.max=80, plot=T, type='correlation')
acf.out = rBDA::cor_fun(df.sine$y, lag.max=1000)
acf.out = rBDA::cor_fun(df.sine$y, lag.max=1000)
acf.out = rBDA::cor_fun(df.sine$y, lag.max=1000)
acf.out$fit_best # lag that gives the highest autocorrelation + the recorded autocorrelation value (excluding a lag of 0)
acf.out = rBDA::cor_fun(flordia60$Time,lag.max = 90)
acf.out = rBDA::cor_fun(flordia60$Time,lag.max = 90)
acf.out = rBDA::cor_fun(flordia60$Time, lag.max = 90)
floridaOUt = rBDA::cor_fun(flordia60$Time, lag.max = 90)
floridaOUt = rBDA::cor_fun(flordia60$Time, lag.max=90)
floridaOut = rBDA::cor_fun(flordia60$Time, lag.max=90)
floridaOut = rBDA::cor_fun(flordia60$Tweets, lag.max=90)
stats::acf(Oregon60$Tweets, lag.max=80, plot=T, type='correlation')
floridaOut$fit_best
floridaOut$wrap
floridaOut$level
floridaOut$type
rBDA::cor_plot(floridaOut, graph = 'line') # the other option (the default) is graph = 'bar'
rBDA::cor_plot(acf.out, graph = 'line') # the other option (the default) is graph = 'bar'
rBDA::cor_plot(floridaOut, graph = 'line') # the other option (the default) is graph = 'bar'
rBDA::cor_plot(acf.out.2)
flordia15 <- subset(tweets15, tweets15$Location == "Florida" )
floridaOut = rBDA::cor_fun(flordia15$Tweets, lag.max=90)
floridaOut$wrap
floridaOut$level
floridaOut$type
rBDA::cor_plot(floridaOut, graph = 'line') # the other option (the default) is graph = 'bar'
View(df.acf)
floridaOut = rBDA::cor_fun(flordia15$Tweets, lag.max=1000)
floridaOut$fit_best
floridaOut$wrap
floridaOut$level
floridaOut$type
rBDA::cor_plot(floridaOut, graph = 'line') # the other option (the default) is graph = 'bar'
floridaOut = rBDA::cor_fun(flordia15$Tweets, lag.max=10)
floridaOut$fit_best
floridaOut$wrap
floridaOut$level
floridaOut$type
rBDA::cor_plot(floridaOut, graph = 'line') # the other option (the default) is graph = 'bar'
floridaOut = rBDA::cor_fun(flordia15$Tweets, lag.max=10000)
floridaOut$wrap
floridaOut$level
floridaOut$type
floridaOut$fit_best
rBDA::cor_plot(floridaOut, graph = 'line') # the other option (the default) is graph = 'bar'
